<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Big Data on GoniGoni!</title>
    <link>/tags/big-data/</link>
    <description>Recent content in Big Data on GoniGoni!</description>
    <generator>Hugo</generator>
    <language>ko-kr</language>
    <lastBuildDate>Mon, 26 Apr 2021 21:20:19 +0900</lastBuildDate>
    <atom:link href="/tags/big-data/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>데이터 분석 워크플로우를 처음부터 만들어 보기 (2)</title>
      <link>/posts/data-workflow-from-scratch-2/</link>
      <pubDate>Mon, 26 Apr 2021 21:20:19 +0900</pubDate>
      <guid>/posts/data-workflow-from-scratch-2/</guid>
      <description>&lt;p&gt;2월에 올렸던 &lt;a href=&#34;/posts/data-workflow-from-scratch/&#34;&gt;데이터 분석 워크플로우를 처음부터 만들어 보기&lt;/a&gt;에 이어서 작성하는 두번째 글입니다.&lt;/p&gt;&#xA;&lt;p&gt;그동안 시도해 봤던 것들은 다음과 같습니다.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;S3 버킷에 원격으로 로그 올리도록 설정하기&lt;/li&gt;&#xA;&lt;li&gt;Airflow 2.x 버전에서 KubernetesExecutor를 사용하는 데, DAG이나 Task를 수동으로 실행할 때 에러가 발생하는 이유는?&lt;/li&gt;&#xA;&lt;li&gt;DAG에서 DB 이용하기: DB와 관련된 Operator 이용하기, Hooks 이용하기&lt;/li&gt;&#xA;&lt;li&gt;S3에서 파일을 가져와서 분석하기: S3Hook&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;테스트 한 환경은 Airflow 2.0.1, 2.0.2 버전입니다.&lt;/p&gt;&#xA;&lt;p&gt;전체 내용은 &lt;a href=&#34;https://github.com/rubysoho07/data-workflow-from-scratch&#34;&gt;GitHub 저장소&lt;/a&gt;에서 확인하실 수 있습니다.&lt;/p&gt;&#xA;&lt;h1 id=&#34;s3-버킷에-원격으로-로그를-올리도록-설정하기&#34;&gt;S3 버킷에 원격으로 로그를 올리도록 설정하기&lt;/h1&gt;&#xA;&lt;p&gt;지난 글에서 시스템 구성으로 KubernetesExecutor를 이용한다고 말씀드렸습니다. KubernetesExecutor를 이용하는 경우, Task가 끝나면 Worker Pod이 사라지면서 로그를 못 찾는 경우도 있기 때문에 로그를 S3 버킷에 저장해 보았습니다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>데이터 분석 워크플로우를 처음부터 만들어 보기</title>
      <link>/posts/data-workflow-from-scratch/</link>
      <pubDate>Mon, 01 Mar 2021 17:00:00 +0900</pubDate>
      <guid>/posts/data-workflow-from-scratch/</guid>
      <description>&lt;p&gt;지난 달에는 데이터 수집을 위한 환경 구성을 처음부터 만들어 보았습니다. 어느 정도 초기 환경을 구축했다고 판단해서, 이번에는 데이터 분석을 위한 워크플로우를 처음부터 만들어 보는 과정을 기록해 보려고 합니다.&lt;/p&gt;&#xA;&lt;p&gt;이번에는 &lt;a href=&#34;http://airflow.apache.org/&#34;&gt;Apache Airflow&lt;/a&gt;를 이용해서 데이터 분석 작업을 위한 환경을 구축해 보았습니다. 여러 곳에서 Airflow를 사용하는 사례를 듣다 보니, Airflow를 한번 써 봐야겠다고 생각했습니다. 최근에는 AWS에서도 &lt;a href=&#34;https://aws.amazon.com/ko/managed-workflows-for-apache-airflow/&#34;&gt;Managed Workflows for Apache Airflow&lt;/a&gt;라는 관리형 서비스를 제공하고 있고, GCP에서는 &lt;a href=&#34;https://cloud.google.com/composer/&#34;&gt;Cloud Composer&lt;/a&gt;라는 이름으로 관리형 서비스를 제공하고 있습니다.&lt;/p&gt;&#xA;&lt;p&gt;사실 전체 아키텍처 구성과 DAG 테스트를 해보고 싶었는데요. 2월이 짧다보니, 우선 기본적인 아키텍처를 이해하기 위해 노력하였습니다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>데이터 수집 단계를 처음부터 구현해 보기</title>
      <link>/posts/collecting-data-from-scratch/</link>
      <pubDate>Sun, 24 Jan 2021 19:36:57 +0900</pubDate>
      <guid>/posts/collecting-data-from-scratch/</guid>
      <description>&lt;p&gt;2021년에는 매월 최소한 글 한 편은 블로그에 올려야겠다고 생각했습니다. 그 달에 공부했던 것, 새로 알게 된 것들을 주로 정리해서 올릴 예정입니다.&lt;/p&gt;&#xA;&lt;p&gt;이번 달은 &lt;strong&gt;&amp;lsquo;데이터 수집 단계를 처음부터 구현해 보기&amp;rsquo;&lt;/strong&gt; 라는 주제로 테스트 해 본 것들을 정리해 보겠습니다.&lt;/p&gt;&#xA;&lt;h2 id=&#34;왜-이-일을-하게-되었나&#34;&gt;왜 이 일을 하게 되었나?&lt;/h2&gt;&#xA;&lt;p&gt;지금 제가 회사에서 하는 일은 학생의 학습 데이터를 수집하고 분석하는 인프라를 구축/운영하는 것입니다. 3년 간 AWS를 기반으로 여러 서비스들을 운영해 보면서, &amp;lsquo;만약 AWS를 쓰지 않는 환경이라면, 어떻게 시스템 구축을 할 것인가?&amp;lsquo;를 고민하게 되었습니다. 그러면서 &amp;lsquo;새로운 방식으로 지금까지 만들었던 것들을 다시 구축해 보면 어떨까?&amp;rsquo; 라는 생각이 들었습니다. 그래서 데이터를 수집하고 분석하는 시스템을 처음부터 구축해 보려고 합니다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hortonworks Sandbox를 AWS에서 사용하기</title>
      <link>/posts/hortonworks-sandbox-on-aws/</link>
      <pubDate>Wed, 16 Oct 2019 21:05:00 +0900</pubDate>
      <guid>/posts/hortonworks-sandbox-on-aws/</guid>
      <description>&lt;h2 id=&#34;들어가며&#34;&gt;들어가며&lt;/h2&gt;&#xA;&lt;p&gt;최근에 &lt;a href=&#34;http://www.yes24.com/Product/Goods/44307209&#34;&gt;&amp;lsquo;하둡과 스파크를 활용한 실용 데이터 과학&amp;rsquo;&lt;/a&gt;이라는 책을 읽고 따라해 보고 있다. 이 책에서는 실습을 위해 호튼웍스(Hortonworks)의 Sandbox 이미지를 사용해 보기를 권장하고 있다. 그런데 설치 방법을 찾다 보니, 권장 사양이 높은 것 같다는 생각이 들었다. 이 이미지를 VirtualBox에서 사용할 때, 메모리 용량이 8GB로 설정되어 있었다. 그런데 지금 내가 쓰고 있는 노트북의 메모리 용량이 8GB라 좀 어려울 것 같았다. 그래서 AWS에 이 이미지를 올려보게 되었다.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;(주의: 이 글에서 설명하는 내용은 AWS의 Free Tier 범위를 넘어가므로 사용한 만큼 요금이 부과됩니다. 크레딧이 있거나 비용 지불이 가능한 경우에 사용하시기 바랍니다. 비용 발생에 대한 책임은 독자에게 있습니다.)&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
