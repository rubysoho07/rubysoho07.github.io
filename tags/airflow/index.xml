<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Airflow on GoniGoni!</title>
    <link>/tags/airflow/</link>
    <description>Recent content in Airflow on GoniGoni!</description>
    <generator>Hugo</generator>
    <language>ko-kr</language>
    <lastBuildDate>Mon, 26 Apr 2021 21:20:19 +0900</lastBuildDate>
    <atom:link href="/tags/airflow/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>데이터 분석 워크플로우를 처음부터 만들어 보기 (2)</title>
      <link>/posts/data-workflow-from-scratch-2/</link>
      <pubDate>Mon, 26 Apr 2021 21:20:19 +0900</pubDate>
      <guid>/posts/data-workflow-from-scratch-2/</guid>
      <description>&lt;p&gt;2월에 올렸던 &lt;a href=&#34;/posts/data-workflow-from-scratch/&#34;&gt;데이터 분석 워크플로우를 처음부터 만들어 보기&lt;/a&gt;에 이어서 작성하는 두번째 글입니다.&lt;/p&gt;&#xA;&lt;p&gt;그동안 시도해 봤던 것들은 다음과 같습니다.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;S3 버킷에 원격으로 로그 올리도록 설정하기&lt;/li&gt;&#xA;&lt;li&gt;Airflow 2.x 버전에서 KubernetesExecutor를 사용하는 데, DAG이나 Task를 수동으로 실행할 때 에러가 발생하는 이유는?&lt;/li&gt;&#xA;&lt;li&gt;DAG에서 DB 이용하기: DB와 관련된 Operator 이용하기, Hooks 이용하기&lt;/li&gt;&#xA;&lt;li&gt;S3에서 파일을 가져와서 분석하기: S3Hook&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;테스트 한 환경은 Airflow 2.0.1, 2.0.2 버전입니다.&lt;/p&gt;&#xA;&lt;p&gt;전체 내용은 &lt;a href=&#34;https://github.com/rubysoho07/data-workflow-from-scratch&#34;&gt;GitHub 저장소&lt;/a&gt;에서 확인하실 수 있습니다.&lt;/p&gt;&#xA;&lt;h1 id=&#34;s3-버킷에-원격으로-로그를-올리도록-설정하기&#34;&gt;S3 버킷에 원격으로 로그를 올리도록 설정하기&lt;/h1&gt;&#xA;&lt;p&gt;지난 글에서 시스템 구성으로 KubernetesExecutor를 이용한다고 말씀드렸습니다. KubernetesExecutor를 이용하는 경우, Task가 끝나면 Worker Pod이 사라지면서 로그를 못 찾는 경우도 있기 때문에 로그를 S3 버킷에 저장해 보았습니다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>데이터 분석 워크플로우를 처음부터 만들어 보기</title>
      <link>/posts/data-workflow-from-scratch/</link>
      <pubDate>Mon, 01 Mar 2021 17:00:00 +0900</pubDate>
      <guid>/posts/data-workflow-from-scratch/</guid>
      <description>&lt;p&gt;지난 달에는 데이터 수집을 위한 환경 구성을 처음부터 만들어 보았습니다. 어느 정도 초기 환경을 구축했다고 판단해서, 이번에는 데이터 분석을 위한 워크플로우를 처음부터 만들어 보는 과정을 기록해 보려고 합니다.&lt;/p&gt;&#xA;&lt;p&gt;이번에는 &lt;a href=&#34;http://airflow.apache.org/&#34;&gt;Apache Airflow&lt;/a&gt;를 이용해서 데이터 분석 작업을 위한 환경을 구축해 보았습니다. 여러 곳에서 Airflow를 사용하는 사례를 듣다 보니, Airflow를 한번 써 봐야겠다고 생각했습니다. 최근에는 AWS에서도 &lt;a href=&#34;https://aws.amazon.com/ko/managed-workflows-for-apache-airflow/&#34;&gt;Managed Workflows for Apache Airflow&lt;/a&gt;라는 관리형 서비스를 제공하고 있고, GCP에서는 &lt;a href=&#34;https://cloud.google.com/composer/&#34;&gt;Cloud Composer&lt;/a&gt;라는 이름으로 관리형 서비스를 제공하고 있습니다.&lt;/p&gt;&#xA;&lt;p&gt;사실 전체 아키텍처 구성과 DAG 테스트를 해보고 싶었는데요. 2월이 짧다보니, 우선 기본적인 아키텍처를 이해하기 위해 노력하였습니다.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
