<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Airflow on GoniGoni!</title>
    <link>/tags/airflow/</link>
    <description>Recent content in Airflow on GoniGoni!</description>
    <generator>Hugo</generator>
    <language>ko-kr</language>
    <lastBuildDate>Mon, 26 Apr 2021 21:20:19 +0900</lastBuildDate>
    <atom:link href="/tags/airflow/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>데이터 분석 워크플로우를 처음부터 만들어 보기 (2)</title>
      <link>/posts/data-workflow-from-scratch-2/</link>
      <pubDate>Mon, 26 Apr 2021 21:20:19 +0900</pubDate>
      <guid>/posts/data-workflow-from-scratch-2/</guid>
      <description>2월에 올렸던 데이터 분석 워크플로우를 처음부터 만들어 보기에 이어서 작성하는 두번째 글입니다.&#xA;그동안 시도해 봤던 것들은 다음과 같습니다.&#xA;S3 버킷에 원격으로 로그 올리도록 설정하기 Airflow 2.x 버전에서 KubernetesExecutor를 사용하는 데, DAG이나 Task를 수동으로 실행할 때 에러가 발생하는 이유는? DAG에서 DB 이용하기: DB와 관련된 Operator 이용하기, Hooks 이용하기 S3에서 파일을 가져와서 분석하기: S3Hook 테스트 한 환경은 Airflow 2.0.1, 2.0.2 버전입니다.&#xA;전체 내용은 GitHub 저장소에서 확인하실 수 있습니다.&#xA;S3 버킷에 원격으로 로그를 올리도록 설정하기 지난 글에서 시스템 구성으로 KubernetesExecutor를 이용한다고 말씀드렸습니다.</description>
    </item>
    <item>
      <title>데이터 분석 워크플로우를 처음부터 만들어 보기</title>
      <link>/posts/data-workflow-from-scratch/</link>
      <pubDate>Mon, 01 Mar 2021 17:00:00 +0900</pubDate>
      <guid>/posts/data-workflow-from-scratch/</guid>
      <description>지난 달에는 데이터 수집을 위한 환경 구성을 처음부터 만들어 보았습니다. 어느 정도 초기 환경을 구축했다고 판단해서, 이번에는 데이터 분석을 위한 워크플로우를 처음부터 만들어 보는 과정을 기록해 보려고 합니다.&#xA;이번에는 Apache Airflow를 이용해서 데이터 분석 작업을 위한 환경을 구축해 보았습니다. 여러 곳에서 Airflow를 사용하는 사례를 듣다 보니, Airflow를 한번 써 봐야겠다고 생각했습니다. 최근에는 AWS에서도 Managed Workflows for Apache Airflow라는 관리형 서비스를 제공하고 있고, GCP에서는 Cloud Composer라는 이름으로 관리형 서비스를 제공하고 있습니다.</description>
    </item>
  </channel>
</rss>
